{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "3f249921-993c-4a46-a059-e22384ab9955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-08 01:14:23--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.6.113, 142.251.6.138, 142.251.6.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.6.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fvdpjrl1jk118bopse1gmlaa0au6rvm2/1654650825000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-06-08 01:14:26--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fvdpjrl1jk118bopse1gmlaa0au6rvm2/1654650825000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 173.194.197.132, 2607:f8b0:4001:c1b::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|173.194.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   156MB/s    in 0.4s    \n",
            "\n",
            "2022-06-08 01:14:27 (156 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "c164e3ba-cff7-4c4a-fd8c-407bcb2b704a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "bee8b223-b825-4152-e944-7c98bfaccdfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "b6023939-ab79-41b9-8ee6-a21109af26e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 8s 7ms/step - loss: 5.9837 - accuracy: 0.0353\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.4300 - accuracy: 0.0353\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3607 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3013 - accuracy: 0.0424\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.2309 - accuracy: 0.0414\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.1627 - accuracy: 0.0419\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0928 - accuracy: 0.0499\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0227 - accuracy: 0.0469\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9547 - accuracy: 0.0600\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8849 - accuracy: 0.0656\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8049 - accuracy: 0.0913\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.7056 - accuracy: 0.0969\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.6072 - accuracy: 0.0964\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5115 - accuracy: 0.1206\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4244 - accuracy: 0.1216\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3388 - accuracy: 0.1317\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2417 - accuracy: 0.1483\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1444 - accuracy: 0.1615\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0538 - accuracy: 0.1796\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9772 - accuracy: 0.1948\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8912 - accuracy: 0.2159\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8002 - accuracy: 0.2311\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7220 - accuracy: 0.2392\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6428 - accuracy: 0.2518\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5522 - accuracy: 0.2830\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4841 - accuracy: 0.2916\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4036 - accuracy: 0.3103\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3302 - accuracy: 0.3396\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2619 - accuracy: 0.3628\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1958 - accuracy: 0.3653\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1293 - accuracy: 0.3900\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0631 - accuracy: 0.4092\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9995 - accuracy: 0.4208\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9423 - accuracy: 0.4268\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8903 - accuracy: 0.4445\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8217 - accuracy: 0.4531\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7856 - accuracy: 0.4546\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7195 - accuracy: 0.4677\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7303 - accuracy: 0.4667\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6625 - accuracy: 0.4783\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5937 - accuracy: 0.4889\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5406 - accuracy: 0.4990\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4777 - accuracy: 0.5040\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4291 - accuracy: 0.5040\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3786 - accuracy: 0.5166\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3251 - accuracy: 0.5262\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2767 - accuracy: 0.5303\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.2324 - accuracy: 0.5333\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1955 - accuracy: 0.5388\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1634 - accuracy: 0.5525\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1190 - accuracy: 0.5590\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1342 - accuracy: 0.5505\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0771 - accuracy: 0.5636\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0291 - accuracy: 0.5762\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9810 - accuracy: 0.5878\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9513 - accuracy: 0.5949\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0171 - accuracy: 0.5691\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0178 - accuracy: 0.5671\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9254 - accuracy: 0.6060\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8515 - accuracy: 0.6130\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8041 - accuracy: 0.6266\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7632 - accuracy: 0.6322\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7309 - accuracy: 0.6428\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7017 - accuracy: 0.6478\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6738 - accuracy: 0.6559\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6558 - accuracy: 0.6554\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6310 - accuracy: 0.6549\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6044 - accuracy: 0.6705\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5784 - accuracy: 0.6796\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.5477 - accuracy: 0.6821\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5255 - accuracy: 0.6867\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4963 - accuracy: 0.6968\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4676 - accuracy: 0.7053\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4438 - accuracy: 0.7084\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4223 - accuracy: 0.7139\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4021 - accuracy: 0.7230\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3878 - accuracy: 0.7255\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3853 - accuracy: 0.7205\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3540 - accuracy: 0.7301\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3342 - accuracy: 0.7336\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3049 - accuracy: 0.7462\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2845 - accuracy: 0.7543\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2923 - accuracy: 0.7432\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2675 - accuracy: 0.7482\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2370 - accuracy: 0.7543\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2125 - accuracy: 0.7598\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1995 - accuracy: 0.7669\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1799 - accuracy: 0.7694\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1526 - accuracy: 0.7765\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1614 - accuracy: 0.7714\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1413 - accuracy: 0.7770\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1516 - accuracy: 0.7775\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1232 - accuracy: 0.7780\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1098 - accuracy: 0.7861\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0753 - accuracy: 0.7957\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0662 - accuracy: 0.7881\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0459 - accuracy: 0.7962\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0657 - accuracy: 0.7876\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0529 - accuracy: 0.7916\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0305 - accuracy: 0.7977\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0075 - accuracy: 0.8047\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9808 - accuracy: 0.8118\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9698 - accuracy: 0.8083\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9566 - accuracy: 0.8098\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9365 - accuracy: 0.8189\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9227 - accuracy: 0.8234\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9153 - accuracy: 0.8234\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9006 - accuracy: 0.8229\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8905 - accuracy: 0.8285\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8795 - accuracy: 0.8274\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8708 - accuracy: 0.8290\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8615 - accuracy: 0.8274\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8771 - accuracy: 0.8259\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8543 - accuracy: 0.8300\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8454 - accuracy: 0.8320\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8306 - accuracy: 0.8310\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8294 - accuracy: 0.8290\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8129 - accuracy: 0.8325\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7985 - accuracy: 0.8340\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7829 - accuracy: 0.8391\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7706 - accuracy: 0.8416\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7620 - accuracy: 0.8396\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7563 - accuracy: 0.8451\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7451 - accuracy: 0.8426\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7386 - accuracy: 0.8431\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8500 - accuracy: 0.8058\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8644 - accuracy: 0.8078\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.8149 - accuracy: 0.8239\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7784 - accuracy: 0.8355\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.7504 - accuracy: 0.8406\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7308 - accuracy: 0.8426\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7143 - accuracy: 0.8522\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7036 - accuracy: 0.8517\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6902 - accuracy: 0.8537\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6819 - accuracy: 0.8557\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6783 - accuracy: 0.8552\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6667 - accuracy: 0.8547\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6606 - accuracy: 0.8623\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6509 - accuracy: 0.8602\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6401 - accuracy: 0.8623\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6326 - accuracy: 0.8532\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6254 - accuracy: 0.8582\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6139 - accuracy: 0.8638\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6044 - accuracy: 0.8658\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5962 - accuracy: 0.8713\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5939 - accuracy: 0.8658\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5921 - accuracy: 0.8673\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5886 - accuracy: 0.8623\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5772 - accuracy: 0.8643\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5674 - accuracy: 0.8724\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5619 - accuracy: 0.8734\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5776 - accuracy: 0.8618\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5856 - accuracy: 0.8708\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6594 - accuracy: 0.8471\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6234 - accuracy: 0.8542\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5885 - accuracy: 0.8572\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5527 - accuracy: 0.8683\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5437 - accuracy: 0.8673\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5300 - accuracy: 0.8784\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5241 - accuracy: 0.8764\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5135 - accuracy: 0.8804\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5091 - accuracy: 0.8779\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5197 - accuracy: 0.8774\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5132 - accuracy: 0.8779\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.8799\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5083 - accuracy: 0.8804\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5070 - accuracy: 0.8789\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5096 - accuracy: 0.8784\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4955 - accuracy: 0.8779\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4846 - accuracy: 0.8855\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4763 - accuracy: 0.8875\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4729 - accuracy: 0.8850\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4688 - accuracy: 0.8865\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4650 - accuracy: 0.8885\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4609 - accuracy: 0.8855\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4601 - accuracy: 0.8860\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4527 - accuracy: 0.8840\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4473 - accuracy: 0.8885\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4440 - accuracy: 0.8870\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4394 - accuracy: 0.8895\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4393 - accuracy: 0.8920\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4397 - accuracy: 0.8895\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4402 - accuracy: 0.8920\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4294 - accuracy: 0.8895\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4249 - accuracy: 0.8940\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4195 - accuracy: 0.8930\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4164 - accuracy: 0.8961\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4138 - accuracy: 0.8910\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4116 - accuracy: 0.8976\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4332 - accuracy: 0.8845\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4315 - accuracy: 0.8824\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4199 - accuracy: 0.8890\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4121 - accuracy: 0.8895\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4049 - accuracy: 0.8915\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4015 - accuracy: 0.8940\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3961 - accuracy: 0.8925\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3948 - accuracy: 0.8905\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4040 - accuracy: 0.8925\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4023 - accuracy: 0.8915\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3984 - accuracy: 0.8951\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "43efd660-dd35-4708-a7a5-7d167709b475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "b5af4551-8585-44ae-9ffd-c74a52f308f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling chills me to youre throwing out is sure to things a ride things talk more walk sleep hes ways ways ways better life break life is joe dreams life else saw dreams dreams you making life making making making making making making making making making making making making making making making making making making sucker ground creep creep creep creep walk scars walk feeling would break making making making making making making making making making making making life new making making making making making grieving throwing night late creep break making making making making life more making life is evening selfish\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}